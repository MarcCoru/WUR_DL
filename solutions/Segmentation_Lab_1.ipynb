{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5962fc5850be971",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Lab 1: Fully Convolutional Networks\n",
    "\n",
    "In this lab, we explore fully convolutional networks for semantic segmentation.\n",
    "\n",
    "## Outline\n",
    "\n",
    "* Run 1: Classification with a ResNet 50 trained on ImageNet\n",
    "* Run 2: Segmentation with a Fully Convolutional ResNet 50 trained on COCO with VOC labels\n",
    "* Run 3: Segmentation with a Deeplab v3 trained on COCO with VOC labels\n",
    "\n",
    "## Setup\n",
    "\n",
    "let's install and import the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f9461aa92c98ba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T19:47:21.440278Z",
     "start_time": "2024-03-24T19:47:15.767714Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision pandas Pillow\n",
    "\n",
    "import urllib\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cd4d240cb8eba7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Test Image\n",
    "\n",
    "we use this image of a dog at test image. \n",
    "\n",
    "**Task**: Take a picture with your phone and load it here. You can use the dog image as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "666ad03b47aa5989",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T19:55:56.532654Z",
     "start_time": "2024-03-24T19:55:54.334967Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "img = Image.open(filename).convert('RGB')\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab4ec027ba5de1c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Run 1: ResNet50 for Classification\n",
    "\n",
    "Let's first predict a class of the image with a ResNet 50 architecture.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/98/ResNet50.png/1024px-ResNet50.png\" width=\"100%\">\n",
    "\n",
    "### ResNet 50\n",
    "\n",
    "ResNet was first proposed in the paper \"Deep Residual Learning for Image Recognition\" by [He et al., 2015](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "ResNet 50 is a deep convolutional neural network architecture renowned for its depth and efficacy in image classification tasks. It introduces skip connections, or residual connections, which facilitate the training of very deep networks by mitigating the vanishing gradient problem. With 50 layers, it achieves state-of-the-art results in tasks like object recognition and scene understanding\n",
    "\n",
    "### ImageNet\n",
    "\n",
    "The ImageNet dataset is a vast collection of labeled images designed for training and evaluating computer vision algorithms. It consists of over 14 million images covering a wide range of categories. Initially, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) focused on 1,000 classes, each representing various objects, animals, scenes, and concepts. These classes include everyday items such as \"car,\" \"dog,\" and \"tree,\" as well as more specific entities like \"golden retriever,\" \"convertible,\" and \"oak tree.\" The diversity of classes in ImageNet allows for comprehensive training and testing of machine learning models for image classification, object detection, and other visual recognition tasks.\n",
    "\n",
    "**Task**:\n",
    "\n",
    "initialize a torchvision ResNet 50 with the ImageNet weights (`ResNet50_Weights.IMAGENET1K_V2`) following the [torchvision documentation](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T20:01:40.653465Z",
     "start_time": "2024-03-24T20:01:40.251017Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "#TODO: Check the documentation and create a resnet50 with ImageNet (IMAGENET1K_V2) weights\n",
    "# weights = ...\n",
    "# resnet50 = ...\n",
    "#SOLUTIONSTART\n",
    "weights = ResNet50_Weights.IMAGENET1K_V2\n",
    "resnet50 = resnet50(weights=weights)\n",
    "#SOLUTIONEND\n",
    "\n",
    "# setting the ResNet to eval() disables dropout and sets BatchNorm layers to training mode\n",
    "resnet50.eval()\n",
    "\n",
    "# print the architecture\n",
    "resnet50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f385276b7d8839e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data preprocessing according to pretraining data\n",
    "\n",
    "Since the model was trained on imagenet weights, we need to make sure that pixel values of new images are similar to the images in the Imagenet dataset.\n",
    "\n",
    "Here, we normalize and resize the image from above to fit the ImageNet statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "788314e58be1c63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T20:01:43.964403Z",
     "start_time": "2024-03-24T20:01:43.957713Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transform = weights.transforms()\n",
    "print(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7321611634c505a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T20:01:45.177217Z",
     "start_time": "2024-03-24T20:01:45.067047Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tensor = transform(img).unsqueeze(0) # transform and add batch dimension\n",
    "\n",
    "print(f\"min: {tensor.min():.2f} max:{tensor.max():.2f}, mean:{tensor.mean():.2f}, std:{tensor.std():.2f}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(tensor.numpy()[0].transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bf19bc6d7629ba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ImageNet categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6efdf344735befdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T20:01:47.309516Z",
     "start_time": "2024-03-24T20:01:47.305579Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories = weights.meta[\"categories\"]\n",
    "# classes\n",
    "print(\", \".join(categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3f7ece3b01d7ce",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Task**: predict a ImageNet with the `resnet50` instance using image `tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fa9217884c5b8d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T19:47:24.250869Z",
     "start_time": "2024-03-24T19:47:24.173608Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad(): # this is optional, but reduces computations (no gradients)\n",
    "    # TODO: predict a class probability using the image tensor and resnet50    \n",
    "    # logits = ...\n",
    "    # y_scores = <todo: softmax(logits)>\n",
    "    \n",
    "    #SOLUTIONSTART\n",
    "    logits = resnet50(tensor)\n",
    "    y_scores = torch.softmax(logits, dim=1)[0]\n",
    "    #SOLUTIONEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3f50eb4813b0063",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T20:05:21.299225Z",
     "start_time": "2024-03-24T20:05:20.547789Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plots probabilities for each of the 1000 imagenet classes\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x=torch.arange(len(categories)).numpy(), height=y_scores.numpy())\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel(\"class ID\")\n",
    "ax.set_ylabel(\"probability\")\n",
    "\n",
    "# lists and sorts classes by probability\n",
    "df = pd.DataFrame([y_scores.numpy(), categories], index=[\"proba\", \"name\"]).T\n",
    "df.sort_values(by=\"proba\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb8a896fb0bef8f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Run 2: Segmentation - FCN ResNet50\n",
    "\n",
    "Let's not move to segmentation with a modified fully convolutional ResNet 50\n",
    "\n",
    "### Fully Convolvolutional ResNet 50\n",
    "\n",
    "The \"Fully Convolutional Networks for Semantic Segmentation\" by [Long et al., 2015](https://arxiv.org/abs/1411.4038) paper presents a modification of classification networks (here ResNet 50), transforming it into a fully convolutional network for semantic segmentation tasks, offering improved performance and efficiency in image understanding. By replacing fully connected layers with convolutional ones, it enables end-to-end pixel-wise predictions, making it suitable for applications like object detection and scene parsing.\n",
    "\n",
    "### Dataset Microsoft COCO with VOC Labels\n",
    "\n",
    "The COCO (Common Objects in Context) dataset is a widely used benchmark for object detection, segmentation, and captioning tasks. It contains over 200,000 labeled images across 80 common object categories such as person, car, and dog, providing rich contextual information for each object instance. While COCO does not directly incorporate VOC (Visual Object Classes) labels, it shares similarities in its goal of advancing object recognition algorithms, albeit with a broader range of object categories and more extensive annotations, making it a valuable resource for training and evaluating computer vision models.\n",
    "\n",
    "**Task**: load a fully convolutional resnet 50 (`fcn_resnet50`) pretrained on the COCO dataset (`COCO_WITH_VOC_LABELS_V1`). Hint: Check the [torchvision documentation](https://pytorch.org/vision/stable/models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4de11c2689a87e7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T20:12:11.225567Z",
     "start_time": "2024-03-24T20:12:09.783355Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "# weights = ...\n",
    "# fcn_resnet50 = ...\n",
    "#SOLUTIONSTART\n",
    "weights = FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\n",
    "fcn_resnet50 = fcn_resnet50(weights=weights)\n",
    "fcn_resnet50.eval()\n",
    "#SOLUTIONEND\n",
    "\n",
    "# Step 2: Initialize the inference transforms (i.e., a preprocess function)\n",
    "# preprocess = ...\n",
    "#SOLUTIONSTART\n",
    "preprocess = weights.transforms()\n",
    "#SOLUTIONEND\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "# tensor = preprocess(...)\n",
    "#SOLUTIONSTART\n",
    "tensor = preprocess(img).unsqueeze(0)\n",
    "#SOLUTIONEND \n",
    "\n",
    "# Step 4: Use the model and predict\n",
    "# logits = ...\n",
    "# probabilities = ...\n",
    "\n",
    "#SOLUTIONSTART\n",
    "logits = fcn_resnet50(tensor)[\"out\"]\n",
    "probabilities = logits.softmax(dim=1)[0].detach()\n",
    "#SOLUTIONEND"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d258de56b13efe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90026ed256f1408",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T20:12:13.811620Z",
     "start_time": "2024-03-24T20:12:12.833302Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories = weights.meta[\"categories\"]\n",
    "print(\", \".join(categories))\n",
    "\n",
    "H, W = 7,3\n",
    "fig, axs = plt.subplots(H,W, figsize=(2*W,2*H))\n",
    "for ax, mask, category in zip(axs.reshape(-1), probabilities, categories):\n",
    "    ax.imshow(mask.numpy())\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(category)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd19807abb869f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's examine the network architecture in comparison to a classification ResNet 50 (Run 1)\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "Compare the classification ResNet50 from above with the Segmentation FCN-ResNet50. What are the differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3f38684db879f29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T20:12:44.158097Z",
     "start_time": "2024-03-24T20:12:44.153169Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fcn_resnet50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd0789661de699b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Run 3: Segmentation - Deeplab V3\n",
    "\n",
    "Let's move away from ResNet and explore Deeplab v3\n",
    "\n",
    "Examine the Deeplab v3 architecture\n",
    "\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-28_at_3.07.48_PM.png\" width=\"100%\"/>\n",
    "\n",
    "\"Rethinking Atrous Convolution for Semantic Image Segmentation\" by [Chen et al., 2017](https://arxiv.org/pdf/1706.05587.pdf) proposes an innovative approach to semantic segmentation using atrous convolution. The paper introduces atrous spatial pyramid pooling (ASPP), which employs multiple atrous rates to capture multi-scale contextual information efficiently. By integrating ASPP into convolutional neural networks, the method achieves significant improvements in semantic segmentation accuracy, particularly in delineating object boundaries and handling objects of varying sizes.\n",
    "\n",
    "**Task**: load a deeplab v3 with resnet50 backbone (i.e., `deeplabv3_resnet50`) pretrained on the COCO dataset (`COCO_WITH_VOC_LABELS_V1`) and predict the image. Hint: Check the four steps from the [torchvision documentation](https://pytorch.org/vision/stable/models.html) and make the visualization below work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e35951fcf25871f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T20:19:31.229987Z",
     "start_time": "2024-03-24T20:19:29.520797Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
    "\n",
    "#TODO: implement the four steps as above (also in the torchvision documentation) and make the visualization code below work.\n",
    "\n",
    "#SOLUTIONSTART\n",
    "weights = DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\n",
    "deeplabv3 = deeplabv3_resnet50(weights=weights)\n",
    "deeplabv3.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "transform = weights.transforms()\n",
    "print(transform)\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "tensor = transform(img).unsqueeze(0)\n",
    "\n",
    "# Step 4: Use the model and visualize the prediction\n",
    "prediction = deeplabv3(tensor)[\"out\"][0]\n",
    "normalized_masks = prediction.softmax(dim=0).detach()\n",
    "#SOLUTIONEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d89f908ca8f2231",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T20:19:37.932577Z",
     "start_time": "2024-03-24T20:19:37.168987Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H, W = 7,3\n",
    "fig, axs = plt.subplots(H,W, figsize=(2*W,2*H))\n",
    "for ax, mask, category in zip(axs.reshape(-1), normalized_masks, categories):\n",
    "    ax.imshow(mask.numpy())\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(category)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b92478e88c663a4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Question 2**\n",
    "\n",
    "Investigate the deeplabv3 architecture. How are are atrous convolutions implemented? What’s the name of the Conv2D hyperparameter? Check the [torch documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93c87f1fccbae46a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T20:19:40.572262Z",
     "start_time": "2024-03-24T20:19:40.560116Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deeplabv3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
